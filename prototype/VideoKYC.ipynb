{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bbdda4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "from deepface.commons import functions\n",
    "import face_recognition as fr\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# dependency configuration\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import docdetect\n",
    "from paddleocr import PaddleOCR,draw_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c81863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundingbox(x1,y1,x2,y2, width, height, scale=1.3, minsize=None):\n",
    "    \"\"\"\n",
    "    based on get_boundingbox method of Andreas RÃ¶ssler\n",
    "    Expects a dlib face to generate a quadratic bounding box.\n",
    "    :param x1 - coordnate from face_recognition get_location \n",
    "    :param y1 - coordnate from face_recognition get_location\n",
    "    :param x2 - coordnate from face_recognition get_location\n",
    "    :param y2 - coordnate from face_recognition get_location\n",
    "    :param width: frame width\n",
    "    :param height: frame height\n",
    "    :param scale: bounding box size multiplier to get a bigger face region\n",
    "    :param minsize: set minimum bounding box size\n",
    "    :return: x, y, bounding_box_size in opencv form\n",
    "    \"\"\"\n",
    "\n",
    "    size_bb = int(max(x2 - x1, y2 - y1) * scale)\n",
    "    if minsize:\n",
    "        if size_bb < minsize:\n",
    "            size_bb = minsize\n",
    "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "    # Check for out of bounds, x-y top left corner\n",
    "    x1 = max(int(center_x - size_bb // 2), 0)\n",
    "    y1 = max(int(center_y - size_bb // 2), 0)\n",
    "    # Check for too big bb size for given x, y\n",
    "    size_bb = min(width - x1, size_bb)\n",
    "    size_bb = min(height - y1, size_bb)\n",
    "\n",
    "    return x1, y1, size_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c3d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_faces(img,face_left_frames, width, height):\n",
    "    try:\n",
    "       \n",
    "        detected_faces = []\n",
    "        face_objs = fr.face_locations(img)            \n",
    "        #locate the faces    \n",
    "        face_detected = False\n",
    "        for face in face_objs:\n",
    "            top, right, bottom, left = face           \n",
    "            x,y,size_bb = get_boundingbox(left,top,right,bottom,width,height)\n",
    "            w=h=size_bb                         \n",
    "            if w > 50:  # discard small detected faces\n",
    "                face_detected = True                \n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (67, 67, 67), 1)  # draw rectangle to main image\n",
    "                cv2.putText(\n",
    "                    img,\n",
    "                    str(face_left_frames),\n",
    "                    (int(x + w / 4), int(y + h / 1.5)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    4,\n",
    "                    (255, 255, 255),\n",
    "                    2,\n",
    "                )                \n",
    "                detected_faces.append((x, y, w, h))\n",
    "    except  Exception as e:  # to avoid exception if no face detected               \n",
    "        detected_faces = []\n",
    "    return detected_faces             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ebf46e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=list(filter(lambda x: x>0.6,[0.1,1,0.4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe06a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = [x for x in df['embedding']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c671a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94314c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist= fr.face_distance(embeddings,embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e64eaa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22a7f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_row(embedding, df, field):\n",
    "\n",
    "    # Get the cosine similarity\n",
    "   # cos_sim = cosine_similarity(np.array(embedding)[None,:], df2[\"embedding\"])\n",
    "   # print(cos_sim)\n",
    "    #dist = np.linalg.norm(list(df2[\"embedding\"]) - np.array(embedding), axis=1)\n",
    "    embeddings = [x for x in df[field]]\n",
    "    #embeddings=df['embedding']\n",
    "    #print(embeddings[0])\n",
    "    #print(embedding)\n",
    "    dist= fr.face_distance(embeddings,embedding)\n",
    "    \n",
    "    print(\"dist\",dist)\n",
    "    filtered_dist = list(filter(lambda x: x<0.6,dist))\n",
    "    if filtered_dist==[]:\n",
    "        return None\n",
    "\n",
    "    # Get the index of the maximum value in the cosine similarity\n",
    "    index = np.argmin(dist)   \n",
    "    # Get the row from df1 with the maximum cosine similarity\n",
    "    row = df.iloc[index]  \n",
    "    # Return the row\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6f52d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32432da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize(img,detected_face,df):\n",
    "\n",
    "    \n",
    "    frameHeight=img.shape[0]\n",
    "    frameWidth=img.shape[1]\n",
    "   \n",
    "    \n",
    "    x = detected_face[0]\n",
    "    y = detected_face[1]\n",
    "    w = detected_face[2]\n",
    "    h = detected_face[3]\n",
    "    custom_face = img[y : y + h, x : x + w]\n",
    "    embedding = fr.face_encodings(custom_face)\n",
    "    \n",
    "    \n",
    "        \n",
    "        #cv2.rectangle(img, (x,y), (x+w,y+h), (0,0,255), int(round(frameHeight/150)), 8)\n",
    "        #embedding = DeepFace.represent(\"data/me_fear.png\", model_name=\"Dlib\")\n",
    "     \n",
    "    row = get_closest_row(embedding[0],df, \"encoding\")\n",
    "    #text = ''#row[\"name\"]+\", \"+row[\"path\"]\n",
    "    #cv2.putText(img, f'{text}', (x, y-10), \n",
    "    #                cv2.FONT_HERSHEY_SIMPLEX, 0.0006*img.shape[1],\n",
    "    #                (255,255,255), int(0.002*img.shape[1]), cv2.LINE_AA)        \n",
    "        \n",
    "        #plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))   \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "989bf135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_summary_box(freeze_img, content, x,y,w,h,resolution_x,resolution_y):\n",
    "    # background of mood box\n",
    "    pivot_img_size=112\n",
    "    # transparency\n",
    "    overlay = freeze_img.copy()\n",
    "    opacity = 0.2\n",
    "    text_box_width = 200\n",
    "    text_box_height = max(h,len(content)*20+20)   \n",
    "   \n",
    "    \n",
    "    cx1 =   x - text_box_width\n",
    "    cx2= x\n",
    "        \n",
    "    if x - text_box_width > 0:\n",
    "        cx1 =   x - text_box_width\n",
    "        cx2= x\n",
    "        # left\n",
    "       \n",
    "    elif x + w + text_box_width < resolution_x:\n",
    "    # right\n",
    "        cx1 =  x + w\n",
    "        cx2=   x + w + text_box_width                                       \n",
    "       \n",
    "    \n",
    "    cv2.rectangle(freeze_img,(cx1, y),(cx2, y + h),(64, 64, 64), cv2.FILLED, )\n",
    "    cv2.addWeighted(overlay, opacity, freeze_img, 1 - opacity, 0, freeze_img)\n",
    "                                                  \n",
    "    index=0\n",
    "    \n",
    "    for key,val in content.items():\n",
    "        \n",
    "        text_location_y = y + 20 + (index ) * 20\n",
    "        text_location_x = cx1\n",
    "\n",
    "        if text_location_y < y + text_box_height:\n",
    "            text = f\"{key}: {val}\"                                      \n",
    "            cv2.putText(freeze_img,text,(text_location_x, text_location_y),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5,(255, 255, 255), 1,)\n",
    "        index+=1    \n",
    "\n",
    "  \n",
    "                       \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb073f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pdocr(freeze_img):   \n",
    "    result = ocr.ocr(freeze_img,cls=False)  \n",
    "    \n",
    "    txts = None if result==[] else [line[1][0] for line in result[0]]  \n",
    "    \n",
    "    scores = None if result==[] else [line[1][1] for line in result[0]] \n",
    "    print(txts,scores)\n",
    "    return txts, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ca0008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_models_for_analysis(wiki=True):\n",
    "\n",
    "    ageProto_wild=\"models/wild/age_deploy.prototxt\"\n",
    "    ageModel_wild=\"models/wild/age_net.caffemodel\"\n",
    "    genderProto_wild=\"models/wild/gender_deploy.prototxt\"\n",
    "    genderModel_wild=\"models/wild/gender_net.caffemodel\"\n",
    "    emoProto =\"models/wild/deploy.prototxt\" \n",
    "    emoModel=\"models/wild/EmotiW_VGG_S.caffemodel\"\n",
    "    \n",
    "    ageProto_wiki=\"models/wiki/age.prototxt\"\n",
    "    ageModel_wiki=\"models/wiki/age.caffemodel\"\n",
    "    genderProto_wiki=\"models/wiki/gender.prototxt\"\n",
    "    genderModel_wiki=\"models/wiki/gender.caffemodel\"\n",
    "\n",
    "    #confidence threshold for face detection\n",
    "    \n",
    "    if wiki:\n",
    "        ageNet=cv2.dnn.readNet(ageModel_wiki,ageProto_wiki)\n",
    "        genderNet=cv2.dnn.readNet(genderModel_wiki,genderProto_wiki)   \n",
    "    else:\n",
    "        print('wild')\n",
    "        ageNet=cv2.dnn.readNet(ageModel_wild,ageProto_wild)\n",
    "        genderNet=cv2.dnn.readNet(genderModel_wild,genderProto_wild)\n",
    "    emoNet =cv2.dnn.readNet(emoModel,emoProto,)\n",
    "    return  ageNet,genderNet,emoNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aeedbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectAgeGender_wild(img,detected_face, ageNet, genderNet):\n",
    "    \n",
    "    \n",
    "    MODEL_MEAN_VALUES=(78.4263377603, 87.7689143744, 114.895847746)\n",
    "    \n",
    "    ageList=['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "    genderList=['M','W']\n",
    "    \n",
    "\n",
    "    frameHeight=img.shape[0]\n",
    "    frameWidth=img.shape[1]\n",
    "   \n",
    "    \n",
    "    x = detected_face[0]\n",
    "    y = detected_face[1]\n",
    "    w = detected_face[2]\n",
    "    h = detected_face[3]\n",
    "    custom_face = img[y : y + h, x : x + w]\n",
    "\n",
    "    blob=cv2.dnn.blobFromImage(custom_face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "    genderNet.setInput(blob)\n",
    "    genderPreds=genderNet.forward()\n",
    "    gender=genderList[genderPreds[0].argmax()]       \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    blob=cv2.dnn.blobFromImage(custom_face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False, crop=False)   \n",
    "    ageNet.setInput(blob)\n",
    "    agePreds=ageNet.forward()\n",
    "    age=ageList[agePreds[0].argmax()]\n",
    "    \n",
    "    return {'age': age, 'gender':gender}\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ced055a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 4/4 [00:04<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "objs = DeepFace.analyze(img_path = \"data/db/leia_organa.jpeg\", detector_backend='dlib',\n",
    "        actions = ['age', 'gender', 'race', 'emotion']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "026c0d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'age': 36,\n",
       "  'region': {'x': 545, 'y': 133, 'w': 155, 'h': 155},\n",
       "  'gender': {'Woman': 99.67048764228821, 'Man': 0.32951917964965105},\n",
       "  'dominant_gender': 'Woman',\n",
       "  'race': {'asian': 1.9868701696395874,\n",
       "   'indian': 3.2996483147144318,\n",
       "   'black': 0.405914057046175,\n",
       "   'white': 47.79939353466034,\n",
       "   'middle eastern': 21.546782553195953,\n",
       "   'latino hispanic': 24.96139258146286},\n",
       "  'dominant_race': 'white',\n",
       "  'emotion': {'angry': 2.029234729707241,\n",
       "   'disgust': 0.0002847034238584456,\n",
       "   'fear': 0.0029691034796996973,\n",
       "   'happy': 84.71994400024414,\n",
       "   'sad': 0.10489042615517974,\n",
       "   'surprise': 0.010533053864492103,\n",
       "   'neutral': 13.132140040397644},\n",
       "  'dominant_emotion': 'happy'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78a6c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectAgeGender_wiki(img,detected_face, ageNet, genderNet):\n",
    "\n",
    "    MODEL_MEAN_VALUES=(78.4263377603, 87.7689143744, 114.895847746)\n",
    "    \n",
    "    frameHeight=img.shape[0]\n",
    "    frameWidth=img.shape[1]\n",
    "   \n",
    "    \n",
    "    x = detected_face[0]\n",
    "    y = detected_face[1]\n",
    "    w = detected_face[2]\n",
    "    h = detected_face[3]\n",
    "    custom_face = img[y : y + h, x : x + w]\n",
    "\n",
    "    \n",
    "    blob=cv2.dnn.blobFromImage(custom_face, 1.0, (224,224), MODEL_MEAN_VALUES, swapRB=False)        \n",
    "    genderNet.setInput(blob)\n",
    "    genderPreds=genderNet.forward()    \n",
    "    gender='W' if np.argmax(genderPreds) == 0 else 'M'      \n",
    "       \n",
    "    \n",
    "    \n",
    "    blob=cv2.dnn.blobFromImage(custom_face, 1.0, (224,224), MODEL_MEAN_VALUES, swapRB=False)\n",
    "    ageNet.setInput(blob)\n",
    "    agePreds=ageNet.forward()    \n",
    "    output_indexes = np.array([i for i in range(0, 101)])\n",
    "    age = int(np.sum(agePreds * output_indexes))      \n",
    "        \n",
    "    \n",
    "    return {'age': age, 'gender':gender}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e3cce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectEmotion(img,detected_face, emoNet):\n",
    "    \n",
    "    conf_threshold=0.7\n",
    "\n",
    "    MODEL_MEAN_VALUES=(78.4263377603, 87.7689143744, 114.895847746)\n",
    "    \n",
    "    #ageList=['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "    #genderList=['M','W']\n",
    "    emoList = [ 'Angry' , 'Disgust' , 'Fear' , 'Happy'  , 'Neutral' ,  'Sad' , 'Surprise']\n",
    "    \n",
    "\n",
    "    frameHeight=img.shape[0]\n",
    "    frameWidth=img.shape[1]\n",
    "   \n",
    "    \n",
    "    x = detected_face[0]\n",
    "    y = detected_face[1]\n",
    "    w = detected_face[2]\n",
    "    h = detected_face[3]\n",
    "    custom_face = img[y : y + h, x : x + w]\n",
    "\n",
    "    #blob=cv2.dnn.blobFromImage(custom_face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "    blob=cv2.dnn.blobFromImage(custom_face, 1.0, (224,224), MODEL_MEAN_VALUES, swapRB=False)\n",
    "        \n",
    "    emoNet.setInput(blob)\n",
    "    emoPreds=emoNet.forward()\n",
    "    emo=emoList[emoPreds[0].argmax()]\n",
    "    #gender=genderList[genderPreds[0].argmax()]       \n",
    "            #print(f'Gender: {gender}')\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "    #cv2.putText(img, f'{gender}, {age}', (x, y-10), \n",
    "    #            cv2.FONT_HERSHEY_SIMPLEX, 0.0009*frameWidth, (255,0,255), int(0.004*frameWidth), cv2.LINE_AA)        \n",
    "    return {'emotion': emo}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b84b9e",
   "metadata": {},
   "source": [
    "### Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89cfb5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "XCEPTION_MODEL = './xception-b5690688.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10a83d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## xception.py\n",
    "\"\"\"\n",
    "Ported to pytorch thanks to [tstandley](https://github.com/tstandley/Xception-PyTorch)\n",
    "@author: tstandley\n",
    "Adapted by cadene\n",
    "Creates an Xception Model as defined in:\n",
    "Francois Chollet\n",
    "Xception: Deep Learning with Depthwise Separable Convolutions\n",
    "https://arxiv.org/pdf/1610.02357.pdf\n",
    "This weights ported from the Keras implementation. Achieves the following performance on the validation set:\n",
    "Loss:0.9173 Prec@1:78.892 Prec@5:94.292\n",
    "REMEMBER to set your image size to 3x299x299 for both test and validation\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                  std=[0.5, 0.5, 0.5])\n",
    "The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.nn import init\n",
    "\n",
    "pretrained_settings = {\n",
    "    'xception': {\n",
    "        'imagenet': {\n",
    "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth',\n",
    "            'input_space': 'RGB',\n",
    "            'input_size': [3, 299, 299],\n",
    "            'input_range': [0, 1],\n",
    "            'mean': [0.5, 0.5, 0.5],\n",
    "            'std': [0.5, 0.5, 0.5],\n",
    "            'num_classes': 1000,\n",
    "            'scale': 0.8975 # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n",
    "        super(SeparableConv2d,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        if out_filters != in_filters or strides!=1:\n",
    "            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n",
    "            self.skipbn = nn.BatchNorm2d(out_filters)\n",
    "        else:\n",
    "            self.skip=None\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        rep=[]\n",
    "\n",
    "        filters=in_filters\n",
    "        if grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "            filters = out_filters\n",
    "\n",
    "        for i in range(reps-1):\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(filters))\n",
    "\n",
    "        if not grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "\n",
    "        if not start_with_relu:\n",
    "            rep = rep[1:]\n",
    "        else:\n",
    "            rep[0] = nn.ReLU(inplace=False)\n",
    "\n",
    "        if strides != 1:\n",
    "            rep.append(nn.MaxPool2d(3,strides,1))\n",
    "        self.rep = nn.Sequential(*rep)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        x = self.rep(inp)\n",
    "\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(inp)\n",
    "            skip = self.skipbn(skip)\n",
    "        else:\n",
    "            skip = inp\n",
    "\n",
    "        x+=skip\n",
    "        return x\n",
    "\n",
    "\n",
    "class Xception(nn.Module):\n",
    "    \"\"\"\n",
    "    Xception optimized for the ImageNet dataset, as specified in\n",
    "    https://arxiv.org/pdf/1610.02357.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=1000):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            num_classes: number of classes\n",
    "        \"\"\"\n",
    "        super(Xception, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        #do relu here\n",
    "\n",
    "        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n",
    "        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n",
    "        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n",
    "\n",
    "        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "\n",
    "        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "\n",
    "        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n",
    "\n",
    "        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n",
    "        self.bn3 = nn.BatchNorm2d(1536)\n",
    "\n",
    "        #do relu here\n",
    "        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n",
    "        self.bn4 = nn.BatchNorm2d(2048)\n",
    "\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "        # #------- init weights --------\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d):\n",
    "        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        #     elif isinstance(m, nn.BatchNorm2d):\n",
    "        #         m.weight.data.fill_(1)\n",
    "        #         m.bias.data.zero_()\n",
    "        # #-----------------------------\n",
    "\n",
    "    def features(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "        x = self.block8(x)\n",
    "        x = self.block9(x)\n",
    "        x = self.block10(x)\n",
    "        x = self.block11(x)\n",
    "        x = self.block12(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        return x\n",
    "\n",
    "    def logits(self, features):\n",
    "        x = self.relu(features)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.features(input)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def xception(num_classes=1000, pretrained='imagenet'):\n",
    "    model = Xception(num_classes=num_classes)\n",
    "    if pretrained:\n",
    "        settings = pretrained_settings['xception'][pretrained]\n",
    "        assert num_classes == settings['num_classes'], \\\n",
    "            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n",
    "\n",
    "        model = Xception(num_classes=num_classes)\n",
    "        model.load_state_dict(model_zoo.load_url(settings['url']))\n",
    "\n",
    "        model.input_space = settings['input_space']\n",
    "        model.input_size = settings['input_size']\n",
    "        model.input_range = settings['input_range']\n",
    "        model.mean = settings['mean']\n",
    "        model.std = settings['std']\n",
    "\n",
    "    # TODO: ugly\n",
    "    model.last_linear = model.fc\n",
    "    del model.fc\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e25c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "## models.py\n",
    "\"\"\"\n",
    "Author: Andreas RÃ¶ssler\n",
    "\"\"\"\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "import torch\n",
    "# import pretrainedmodels\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from network.xception import xception\n",
    "import math\n",
    "import torchvision\n",
    "\n",
    "\n",
    "def return_pytorch04_xception(pretrained=True):\n",
    "    # Raises warning \"src not broadcastable to dst\" but thats fine\n",
    "    model = xception(pretrained=False)\n",
    "    if pretrained:\n",
    "        # Load model in torch 0.4+\n",
    "        model.fc = model.last_linear\n",
    "        del model.last_linear\n",
    "        state_dict = torch.load(\n",
    "            #'/home/ondyari/.torch/models/xception-b5690688.pth')\n",
    "            XCEPTION_MODEL)\n",
    "        for name, weights in state_dict.items():\n",
    "            if 'pointwise' in name:\n",
    "                state_dict[name] = weights.unsqueeze(-1).unsqueeze(-1)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.last_linear = model.fc\n",
    "        del model.fc\n",
    "    return model\n",
    "\n",
    "\n",
    "class TransferModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple transfer learning model that takes an imagenet pretrained model with\n",
    "    a fc layer as base model and retrains a new fc layer for num_out_classes\n",
    "    \"\"\"\n",
    "    def __init__(self, modelchoice, num_out_classes=2, dropout=0.0):\n",
    "        super(TransferModel, self).__init__()\n",
    "        self.modelchoice = modelchoice\n",
    "        if modelchoice == 'xception':\n",
    "            self.model = return_pytorch04_xception()\n",
    "            # Replace fc\n",
    "            num_ftrs = self.model.last_linear.in_features\n",
    "            if not dropout:\n",
    "                self.model.last_linear = nn.Linear(num_ftrs, num_out_classes)\n",
    "            else:\n",
    "                print('Using dropout', dropout)\n",
    "                self.model.last_linear = nn.Sequential(\n",
    "                    nn.Dropout(p=dropout),\n",
    "                    nn.Linear(num_ftrs, num_out_classes)\n",
    "                )\n",
    "        elif modelchoice == 'resnet50' or modelchoice == 'resnet18':\n",
    "            if modelchoice == 'resnet50':\n",
    "                self.model = torchvision.models.resnet50(pretrained=True)\n",
    "            if modelchoice == 'resnet18':\n",
    "                self.model = torchvision.models.resnet18(pretrained=True)\n",
    "            # Replace fc\n",
    "            num_ftrs = self.model.fc.in_features\n",
    "            if not dropout:\n",
    "                self.model.fc = nn.Linear(num_ftrs, num_out_classes)\n",
    "            else:\n",
    "                self.model.fc = nn.Sequential(\n",
    "                    nn.Dropout(p=dropout),\n",
    "                    nn.Linear(num_ftrs, num_out_classes)\n",
    "                )\n",
    "        else:\n",
    "            raise Exception('Choose valid model, e.g. resnet50')\n",
    "\n",
    "    def set_trainable_up_to(self, boolean, layername=\"Conv2d_4a_3x3\"):\n",
    "        \"\"\"\n",
    "        Freezes all layers below a specific layer and sets the following layers\n",
    "        to true if boolean else only the fully connected final layer\n",
    "        :param boolean:\n",
    "        :param layername: depends on network, for inception e.g. Conv2d_4a_3x3\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Stage-1: freeze all the layers\n",
    "        if layername is None:\n",
    "            for i, param in self.model.named_parameters():\n",
    "                param.requires_grad = True\n",
    "                return\n",
    "        else:\n",
    "            for i, param in self.model.named_parameters():\n",
    "                param.requires_grad = False\n",
    "        if boolean:\n",
    "            # Make all layers following the layername layer trainable\n",
    "            ct = []\n",
    "            found = False\n",
    "            for name, child in self.model.named_children():\n",
    "                if layername in ct:\n",
    "                    found = True\n",
    "                    for params in child.parameters():\n",
    "                        params.requires_grad = True\n",
    "                ct.append(name)\n",
    "            if not found:\n",
    "                raise Exception('Layer not found, cant finetune!'.format(\n",
    "                    layername))\n",
    "        else:\n",
    "            if self.modelchoice == 'xception':\n",
    "                # Make fc trainable\n",
    "                for param in self.model.last_linear.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            else:\n",
    "                # Make fc trainable\n",
    "                for param in self.model.fc.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def model_selection(modelname, num_out_classes,\n",
    "                    dropout=None):\n",
    "    \"\"\"\n",
    "    :param modelname:\n",
    "    :return: model, image size, pretraining<yes/no>, input_list\n",
    "    \"\"\"\n",
    "    if modelname == 'xception':\n",
    "        return TransferModel(modelchoice='xception',\n",
    "                             num_out_classes=num_out_classes), 299, \\\n",
    "               True, ['image'], None\n",
    "    elif modelname == 'resnet18':\n",
    "        return TransferModel(modelchoice='resnet18', dropout=dropout,\n",
    "                             num_out_classes=num_out_classes), \\\n",
    "               224, True, ['image'], None\n",
    "    else:\n",
    "        raise NotImplementedError(modelname)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     model, image_size, *_ = model_selection('resnet18', num_out_classes=2)\n",
    "#     print(model)\n",
    "#     model = model.cuda()\n",
    "#     from torchsummary import summary\n",
    "#     input_s = (3, image_size, image_size)\n",
    "#     print(summary(model, input_s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af7a2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform.py\n",
    "\"\"\"\n",
    "Author: Andreas RÃ¶ssler\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from torchvision import transforms\n",
    "\n",
    "xception_default_data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62e16942",
   "metadata": {},
   "outputs": [],
   "source": [
    "## detect_from_video.py\n",
    "\"\"\"\n",
    "Evaluates a folder of video files or a single file with a xception binary\n",
    "classification network.\n",
    "Usage:\n",
    "python detect_from_video.py\n",
    "    -i <folder with video files or path to video file>\n",
    "    -m <path to model file>\n",
    "    -o <path to output folder, will write one or multiple output videos there>\n",
    "Author: Andreas RÃ¶ssler\n",
    "\"\"\"\n",
    "import os\n",
    "import argparse\n",
    "from os.path import join\n",
    "import cv2\n",
    "import dlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image as pil_image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def preprocess_image(image, cuda=True):\n",
    "    \"\"\"\n",
    "    Preprocesses the image such that it can be fed into our network.\n",
    "    During this process we envoke PIL to cast it into a PIL image.\n",
    "    :param image: numpy image in opencv form (i.e., BGR and of shape\n",
    "    :return: pytorch tensor of shape [1, 3, image_size, image_size], not\n",
    "    necessarily casted to cuda\n",
    "    \"\"\"\n",
    "    # Revert from BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Preprocess using the preprocessing function used during training and\n",
    "    # casting it to PIL image\n",
    "    preprocess = xception_default_data_transforms['test']\n",
    "    preprocessed_image = preprocess(pil_image.fromarray(image))\n",
    "    # Add first dimension as the network expects a batch\n",
    "    preprocessed_image = preprocessed_image.unsqueeze(0)\n",
    "    if cuda:\n",
    "        preprocessed_image = preprocessed_image.cuda()\n",
    "    return preprocessed_image\n",
    "\n",
    "\n",
    "def predict_with_model(image, model, post_function=nn.Softmax(dim=1),\n",
    "                       cuda=True):\n",
    "    \"\"\"\n",
    "    Predicts the label of an input image. Preprocesses the input image and\n",
    "    casts it to cuda if required\n",
    "    :param image: numpy image\n",
    "    :param model: torch model with linear layer at the end\n",
    "    :param post_function: e.g., softmax\n",
    "    :param cuda: enables cuda, must be the same parameter as the model\n",
    "    :return: prediction (1 = fake, 0 = real)\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    preprocessed_image = preprocess_image(image, cuda)\n",
    "\n",
    "    # Model prediction\n",
    "    output = model(preprocessed_image)\n",
    "    output = post_function(output)\n",
    "\n",
    "    # Cast to desired\n",
    "    _, prediction = torch.max(output, 1)    # argmax\n",
    "    prediction = float(prediction.cpu().numpy())\n",
    "\n",
    "    return int(prediction), output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f74a0252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_frame_pred(img, detected_face,model,cuda=True):\n",
    "    \"\"\"\n",
    "    Predict and give result as numpy array\n",
    "    \"\"\"\n",
    "    frameHeight=img.shape[0]\n",
    "    frameWidth=img.shape[1]\n",
    "   \n",
    "    \n",
    "    x = detected_face[0]\n",
    "    y = detected_face[1]\n",
    "    w = detected_face[2]\n",
    "    h = detected_face[3]\n",
    "    custom_face = img[y : y + h, x : x + w]\n",
    "   \n",
    "    \n",
    "    prediction = 0\n",
    "    output = []   \n",
    "    # Actual prediction using our model\n",
    "    prediction, output = predict_with_model(custom_face, model,\n",
    "                                                        cuda=cuda)\n",
    "    # ------------------------------------------------------------------\n",
    "        \n",
    "    return prediction, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2be5653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_full_c23 = '../deepfake/faceforensics_models/faceforensics++_models_subset/full/xception/full_c23.p'\n",
    "model_full_c23 = torch.load(model_path_full_c23, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7af15a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_full_c40 = '../deepfake/faceforensics_models/faceforensics++_models_subset/full/xception/full_c40.p'\n",
    "model_full_c40 = torch.load(model_path_full_c40, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea39ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_smile(landmarks):    \n",
    "    # Calculate the lips width    \n",
    "    left_lip_edge = np.array(landmarks['top_lip'][0])\n",
    "    right_lip_edge = np.array(landmarks['top_lip'][6])\n",
    "    lips_width = np.abs(left_lip_edge[0]-right_lip_edge[0])    \n",
    "    face_width = np.abs(landmarks['chin'][2][0] - landmarks['chin'][14][0])   \n",
    "    print(lips_width,face_width,lips_width/face_width)\n",
    "    return  lips_width/face_width >=0.45   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05be40ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fr_smile_detect(img,detected_face,landmarks):        \n",
    "    frameHeight=img.shape[0]\n",
    "    frameWidth=img.shape[1]\n",
    "   \n",
    "    \n",
    "    x = detected_face[0]\n",
    "    y = detected_face[1]\n",
    "    w = detected_face[2]\n",
    "    h = detected_face[3]\n",
    "    custom_face = img[y : y + h, x : x + w]\n",
    "           \n",
    "    smiled = is_smile(landmarks)              \n",
    "        \n",
    "    return smiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff699af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel = load_model('../liveness/data/left_eye.h5')\n",
    "rmodel = load_model('../liveness/data/right_eye.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2d349aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eyes(frame, landmarks):\n",
    "    x1,y1 = landmarks['left_eye'][0][0], landmarks['left_eye'][1][1]\n",
    "    x2,y2 = landmarks['left_eye'][3][0], landmarks['left_eye'][5][1]\n",
    "    shape1 = [(x1-5,y1-5), (x2+5,y2+5)]\n",
    "        \n",
    "    x11,y11 = landmarks['right_eye'][0][0], landmarks['right_eye'][1][1]\n",
    "    x21,y21 = landmarks['right_eye'][3][0], landmarks['right_eye'][5][1]\n",
    "    shape2 = [(x11-5,y11-5), (x21+5,y21+5)]\n",
    "        \n",
    "    left_eye = frame[shape1[0][1]:shape1[1][1],shape1[0][0]:shape1[1][0],:]\n",
    "    right_eye = frame[shape2[0][1]:shape2[1][1],shape2[0][0]:shape2[1][0],:]\n",
    "    return (left_eye, right_eye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b158a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fr_dnn_eye_roll_detect(img,detected_face,landmarks):        \n",
    "    frameHeight=img.shape[0]\n",
    "    frameWidth=img.shape[1]\n",
    "   \n",
    "    \n",
    "    x = detected_face[0]\n",
    "    y = detected_face[1]\n",
    "    w = detected_face[2]\n",
    "    h = detected_face[3]\n",
    "    custom_face = img[y : y + h, x : x + w]     \n",
    "    left_eye, right_eye = get_eyes(img,landmarks)\n",
    "    l_img =  cv2.resize(left_eye, (30,20), interpolation = cv2.INTER_AREA)        \n",
    "    x = np.expand_dims(l_img, axis=0)\n",
    "    images = np.vstack([x])       \n",
    "    l_prediction = lmodel.predict(images, batch_size=10)    \n",
    "        \n",
    "    r_img =  cv2.resize(right_eye, (30,20), interpolation = cv2.INTER_AREA)        \n",
    "    x = np.expand_dims(r_img, axis=0)\n",
    "    images = np.vstack([x])       \n",
    "    r_prediction = rmodel.predict(images, batch_size=10)       \n",
    "                \n",
    "    eyes_rolled_flag =  l_prediction[0] >=0.5 and r_prediction[0] >=0.5\n",
    "    return eyes_rolled_flag\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea6f0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0d83ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = {\"id\": [1,2,3],\n",
    "      \"name\": [\"Elina Maliarsky\", \"Leia Organa\",'Elina Maliarsky'],\n",
    "      \"path\" : [\"data/db/me_fear.png\",\"data/db/leia_organa.jpeg\",\"data/db/me_happy.png\"] \n",
    "      \n",
    "     }\n",
    "\n",
    "df1=pd.DataFrame(db1)\n",
    "\n",
    "df1[\"encoding\"] = df1.apply(lambda r: DeepFace.represent(r[\"path\"], model_name='Dlib')[0]['embedding'] ,axis=1)\n",
    "\n",
    "db2 = {\"id\": [1],\n",
    "      \"name\": [ \"Leia Organa\"],\n",
    "      \"path\" : [\"data/db/leia_organa.jpeg\"] \n",
    "      \n",
    "     }\n",
    "\n",
    "df2=pd.DataFrame(db2)\n",
    "\n",
    "df2[\"encoding\"]=df2.apply(lambda r: DeepFace.represent(r[\"path\"], model_name='Dlib')[0]['embedding'] ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29d8b611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Elina Maliarsky</td>\n",
       "      <td>data/db/me_fear.png</td>\n",
       "      <td>[-0.087116539478302, 0.14015552401542664, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Leia Organa</td>\n",
       "      <td>data/db/leia_organa.jpeg</td>\n",
       "      <td>[-0.03101312555372715, -0.0030283723026514053,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Elina Maliarsky</td>\n",
       "      <td>data/db/me_happy.png</td>\n",
       "      <td>[-0.10627612471580505, 0.1906319409608841, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id             name                      path  \\\n",
       "0   1  Elina Maliarsky       data/db/me_fear.png   \n",
       "1   2      Leia Organa  data/db/leia_organa.jpeg   \n",
       "2   3  Elina Maliarsky      data/db/me_happy.png   \n",
       "\n",
       "                                            encoding  \n",
       "0  [-0.087116539478302, 0.14015552401542664, 0.06...  \n",
       "1  [-0.03101312555372715, -0.0030283723026514053,...  \n",
       "2  [-0.10627612471580505, 0.1906319409608841, 0.0...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6198c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a96001e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings[0][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d79637ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_verification = {\"id\": [\"12345\"],\n",
    "      \"first_name\": [\"ELINA\"],\n",
    "      \"last_name\" : [\"MALIARSKY\"]            \n",
    "     }\n",
    "\n",
    "df_verification=pd.DataFrame(db_verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a96da801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, first_name, last_name]\n",
       "Index: []"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_verification[df_verification['first_name'].str.casefold()!='elina'.casefold()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07420c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_identity(df,ID,first_name, last_name):\n",
    "    row = df[(df['id'].str.lower() ==ID.lower()) & \n",
    "             ((df['first_name'].str.lower() == first_name.lower())\n",
    "              | (df['last_name'].str.lower() == last_name.lower())) ]\n",
    "    \n",
    "    return len(row)>0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4ab8bd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023/05/03 10:51:56] ppocr DEBUG: Namespace(alpha=1.0, benchmark=False, beta=1.0, cls_batch_num=6, cls_image_shape='3, 48, 192', cls_model_dir='C:\\\\Users\\\\elinam/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_thresh=0.9, cpu_threads=10, crop_res_save_dir='./output', det=True, det_algorithm='DB', det_box_type='quad', det_db_box_thresh=0.6, det_db_score_mode='fast', det_db_thresh=0.3, det_db_unclip_ratio=1.5, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_east_score_thresh=0.8, det_limit_side_len=960, det_limit_type='max', det_model_dir='C:\\\\Users\\\\elinam/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, det_pse_thresh=0, det_sast_nms_thresh=0.2, det_sast_score_thresh=0.5, draw_img_save_dir='./inference_results', drop_score=0.5, e2e_algorithm='PGNet', e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_limit_side_len=768, e2e_limit_type='max', e2e_model_dir=None, e2e_pgnet_mode='fast', e2e_pgnet_score_thresh=0.5, e2e_pgnet_valid_set='totaltext', enable_mkldnn=False, fourier_degree=5, gpu_mem=500, help='==SUPPRESS==', image_dir=None, image_orientation=False, ir_optim=True, kie_algorithm='LayoutXLM', label_list=['0', '180'], lang='en', layout=True, layout_dict_path=None, layout_model_dir=None, layout_nms_threshold=0.5, layout_score_threshold=0.5, max_batch_size=10, max_text_length=25, merge_no_span_structure=True, min_subgraph_size=15, mode='structure', ocr=True, ocr_order_method=None, ocr_version='PP-OCRv3', output='./output', page_num=0, precision='fp32', process_id=0, re_model_dir=None, rec=True, rec_algorithm='SVTR_LCNet', rec_batch_num=6, rec_char_dict_path='C:\\\\Users\\\\elinam\\\\.conda\\\\envs\\\\nlp_cv\\\\lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\en_dict.txt', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_model_dir='C:\\\\Users\\\\elinam/.paddleocr/whl\\\\rec\\\\en\\\\en_PP-OCRv3_rec_infer', recovery=False, save_crop_res=False, save_log_path='./log_output/', scales=[8, 16, 32], ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ser_model_dir=None, show_log=True, sr_batch_num=1, sr_image_shape='3, 32, 128', sr_model_dir=None, structure_version='PP-StructureV2', table=True, table_algorithm='TableAttn', table_char_dict_path=None, table_max_len=488, table_model_dir=None, total_process_num=1, type='ocr', use_angle_cls=False, use_dilation=False, use_gpu=False, use_mp=False, use_npu=False, use_onnx=False, use_pdf2docx_api=False, use_pdserving=False, use_space_char=True, use_tensorrt=False, use_visual_backbone=True, use_xpu=False, vis_font_path='./doc/fonts/simfang.ttf', warmup=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tic = time.time()\n",
    "cap = cv2.VideoCapture(2)\n",
    "freeze = False\n",
    "face_detected = False\n",
    "face_included_frames = 0  # freeze screen if face detected sequantially 5 frames\n",
    "freezed_frame = 0\n",
    "time_threshold=5\n",
    "frame_threshold = 5\n",
    "flag_detect= False\n",
    "flag_recognize = False\n",
    "flag_match=False\n",
    "flag_liveness = False\n",
    "flag_deepfake = False\n",
    "flag_analyze = False\n",
    "df_rec = df1.copy()\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#cv2.namedWindow('img',cv2.WINDOW_NORMAL)\n",
    "#cv2.resizeWindow('img', 775,600)\n",
    "out = cv2.VideoWriter('demo_kyc4.avi', fourcc, 20.0, (640,  480))\n",
    "pivot_img_size = 112  # face recognition result image\n",
    "\n",
    "ageNet,genderNet,emoNet = prepare_models_for_analysis(True)\n",
    "\n",
    "\n",
    "# need to run only once to download and load model into memory\n",
    "ocr = PaddleOCR(lang='en')\n",
    "frame_id=0\n",
    "df_detect=pd.DataFrame(columns = [\"frame_id\", \"age\",\"gender\",\"deepfake\",\"encoding\", \"name\"])\n",
    "while True:\n",
    "    _, img = cap.read()\n",
    "    if img is None:\n",
    "        break\n",
    "    raw_img = img.copy()\n",
    "    resolution_x = img.shape[1]\n",
    "    resolution_y = img.shape[0]\n",
    "    \n",
    "    key=cv2.waitKey(1)\n",
    "    \n",
    "    #if cv2.waitKey(1) & 0xFF == ord(\"a\"):  # press q to quit\n",
    "    #detecting unknown face\n",
    "    if key==ord(\"d\"):\n",
    "        flag_detect=True \n",
    "        flag_analyze=True\n",
    "        flag_deepfake=True\n",
    "    #detecting known_face    \n",
    "    if key==ord(\"r\"):\n",
    "        flag_detect=True \n",
    "        flag_recognize=True\n",
    "        flag_deepfake=True    \n",
    "    if key==ord(\"m\"):\n",
    "        flag_match=True            \n",
    "    if key==ord(\"l\"):\n",
    "        flag_liveness=True\n",
    "        \n",
    "    \n",
    "    \n",
    "    #if flags for detection/recognition/liveness are on\n",
    "    if flag_detect or flag_match or flag_liveness:  \n",
    "        detected_faces = []\n",
    "        if freeze == False:\n",
    "            \n",
    "            frames_left = frame_threshold - face_included_frames-1\n",
    "            detected_faces =  locate_faces(img,frames_left, resolution_x, resolution_y)            \n",
    "            if len(detected_faces)>0:\n",
    "                face_detected=True\n",
    "                face_included_frames+=1\n",
    "        else:\n",
    "            detected_faces = []\n",
    "            \n",
    "        \n",
    "        if face_detected  and not freeze and face_included_frames == frame_threshold: \n",
    "            print(\"tofreeze\")\n",
    "            freeze = True\n",
    "            #base_img = img.copy()\n",
    "            base_img = raw_img.copy()\n",
    "            detected_faces_final = detected_faces.copy()\n",
    "            tic = time.time()\n",
    "\n",
    "        if freeze == True:            \n",
    "            toc = time.time()\n",
    "            if (toc - tic) < time_threshold:\n",
    "\n",
    "                if freezed_frame == 0:\n",
    "                    #detection: always done\n",
    "                    freeze_img = base_img.copy()\n",
    "                    # here, np.uint8 handles showing white area issue\n",
    "                    # freeze_img = np.zeros(resolution, np.uint8)                \n",
    "\n",
    "                    for detected_face in detected_faces_final:\n",
    "                        attributes={}\n",
    "                        x = detected_face[0]\n",
    "                        y = detected_face[1]\n",
    "                        w = detected_face[2]\n",
    "                        h = detected_face[3]\n",
    "                        \n",
    "                        # -------------------------------\n",
    "                        # extract detected face\n",
    "                        custom_face = base_img[y : y + h, x : x + w]\n",
    "                        \n",
    "                        # draw rectangle to main image\n",
    "                        cv2.rectangle(\n",
    "                            freeze_img, (x, y), (x + w, y + h), (67, 67, 67), 1\n",
    "                        )\n",
    "                        \n",
    "                        #path=''\n",
    "                        #recognition\n",
    "                        if(flag_recognize):\n",
    "                            print('recognize')\n",
    "                            row=recognize(freeze_img,detected_face,df_rec)\n",
    "                            #print(fid)\n",
    "                            attributes['recognized']= \"Yes\" if row is not None else \"No\"\n",
    "                            if row is None:\n",
    "                                flag_analyze=True\n",
    "                            else:\n",
    "                                attributes['name']=row['name']    \n",
    "                                #attributes['path']=row['path'] \n",
    "                                #maybe portait\n",
    "                            \n",
    "                        # facial attribute analysis   \n",
    "                        if(flag_analyze):\n",
    "                            #deepface\n",
    "                            #age_gender = DeepFace.analyze(actions = ['age','gender'],img_path=custom_face,\n",
    "                            #    detector_backend='dlib',\n",
    "                            #    enforce_detection=False,\n",
    "                            #    silent=True,)\n",
    "                            #attributes['age'] = age_gender[0]['age']\n",
    "                            #attributes['gender'] = age_gender[0]['dominant_gender']\n",
    "                            \n",
    "                            #custom age_gender\n",
    "                            age_gender = detectAgeGender_wiki(freeze_img,detected_face,ageNet,genderNet)\n",
    "                            for key in age_gender.keys():\n",
    "                                attributes[key] =age_gender[key]\n",
    "                                \n",
    "                           \n",
    "                        \n",
    "                        #face forgery detection\n",
    "                        if(flag_deepfake):    \n",
    "                            prediction, output = image_frame_pred(freeze_img,detected_face, model_full_c23,\n",
    "                                                    cuda=False)           \n",
    "                       \n",
    "                            output_list = ['{0:.2f}'.format(float(x)) for x in\n",
    "                            output.detach().cpu().numpy()[0]]\n",
    "                            print(output_list, prediction)\n",
    "                            attributes[\"deepfake\"]=\"detected\" if prediction==1 else \"not detected\"\n",
    "                        \n",
    "                        \n",
    "                        #liveness detection\n",
    "                        if(flag_liveness):\n",
    "                            landmarks_collection = fr.face_landmarks(freeze_img)\n",
    "                            smile=fr_smile_detect(freeze_img, detected_face, landmarks_collection[0])       \n",
    "                            attributes[\"smiled\"] = \"Yes\" if smile else \"No\" \n",
    "        \n",
    "                            #eye roll detection\n",
    "                            er=fr_dnn_eye_roll_detect(freeze_img, detected_face,landmarks_collection[0])\n",
    "                            attributes[\"rolled eyes\"] = \"Yes\" if er else \"No\"\n",
    "                            \n",
    "                            \n",
    "                            #emo = DeepFace.analyze(actions = ['emotion'],img_path=custom_face,\n",
    "                            #    detector_backend='dlib',\n",
    "                            #    enforce_detection=False,\n",
    "                            #    silent=True,)\n",
    "                            #attributes['emotion'] = emo[0]['dominant_emotion']\n",
    "                            \n",
    "                            emo = detectEmotion(freeze_img,detected_face,emoNet)\n",
    "                            attributes[\"emotion\"] = emo['emotion']\n",
    "                        \n",
    "                                                \n",
    "                                                 \n",
    "                           \n",
    "                        if(flag_detect or flag_analyze or flag_deepfake):\n",
    "                            df_row= {\"frame_id\":frame_id,\n",
    "                                     \"age\":attributes.get(\"age\", '')\n",
    "                                   ,\"gender\":attributes.get(\"gender\", ''),\n",
    "                                    \"name\" : attributes.get(\"name\", '') ,     \n",
    "                                   \"deepfake\":attributes.get(\"deepfake\", '')                                     \n",
    "                               ,\"encoding\": fr.face_encodings(custom_face)[0]}\n",
    "                            df_detect=df_detect.append(df_row, ignore_index = True) \n",
    "                            print(frame_id)\n",
    "                            print(df_detect)\n",
    "                    \n",
    "                        # --------------------------------\n",
    "                        # face recognition\n",
    "                        # call find function for custom_face\n",
    "                        if(flag_match):\n",
    "                            row=recognize(freeze_img,detected_face,df_detect)\n",
    "                            attributes['matched']= \"Yes\" if row is not None else \"No\"\n",
    "                            texts,_=do_pdocr(freeze_img)\n",
    "                            verified = True\n",
    "                            if len(texts)<3:\n",
    "                                verified = False\n",
    "                            else:\n",
    "                                print(df_verification)\n",
    "                                verified = verify_identity(df_verification,texts[0],texts[1],texts[2])\n",
    "                                attributes['ID']= texts[0]\n",
    "                                attributes['Name']= f'{texts[1]} {texts[2]}'\n",
    "                                \n",
    "                            attributes['Verified']=\"Yes\" if verified else \"No\"     \n",
    "                            print(attributes['Verified'])\n",
    "                            \n",
    "                        \n",
    "                        draw_summary_box(freeze_img,attributes,x,y,w,h,resolution_x,resolution_y)  \n",
    "                                   \n",
    "                        \n",
    "                                \n",
    "                        tic = time.time()  # in this way, freezed image can show 5 seconds\n",
    "                        \n",
    "\n",
    "                        # -------------------------------\n",
    "\n",
    "                time_left = int(time_threshold - (toc - tic) + 1)\n",
    "\n",
    "                cv2.rectangle(freeze_img, (10, 10), (90, 50), (67, 67, 67), -10)\n",
    "                cv2.putText(\n",
    "                    freeze_img,\n",
    "                    str(time_left),\n",
    "                    (40, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1,\n",
    "                    (255, 255, 255),\n",
    "                    1,\n",
    "                )\n",
    "                out.write(freeze_img)\n",
    "                cv2.imshow(\"img\", freeze_img)\n",
    "\n",
    "                freezed_frame +=1\n",
    "            else:\n",
    "                face_detected = False\n",
    "                face_included_frames = 0\n",
    "                freezed_frame=0\n",
    "                freeze = False\n",
    "                flag_detect= False\n",
    "                flag_recognize = False\n",
    "                flag_match = False\n",
    "                flag_liveness = False\n",
    "                flag_deepfake = False\n",
    "                flag_analyze = False\n",
    "                flag_OCR=False\n",
    "               \n",
    "\n",
    "        else:\n",
    "            out.write(img)\n",
    "            cv2.imshow(\"img\", img)\n",
    "    else:\n",
    "        out.write(img)\n",
    "        cv2.imshow(\"img\", img)\n",
    "                \n",
    "    frame_id+=1\n",
    "    if key == ord(\"q\"):  # press q to quit\n",
    "        break\n",
    "\n",
    "    # kill open cv things\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b958ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # kill open cv things\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-nlp_cv] *",
   "language": "python",
   "name": "conda-env-.conda-nlp_cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
